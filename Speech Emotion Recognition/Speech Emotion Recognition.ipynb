{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["FZdwQYlRds_z","Fr2d1lBeQL2J","ZSYdpvlzQJiP"],"mount_file_id":"1iPoxjLel1b09HePLToKr6r1XRrUZVEBk","authorship_tag":"ABX9TyODfFIpvvQ+MACqT3RjNeUa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/drive/1iPoxjLel1b09HePLToKr6r1XRrUZVEBk?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open This File In Colab \"/></a>"],"metadata":{"id":"Vm8cX8DTaDg5"}},{"cell_type":"markdown","metadata":{"id":"wd1zknE2IRew"},"source":["\n","\n","\n","# **[Dataset](https://drive.google.com/drive/folders/1ORGXLuAauH1yoHLh31g8f3HMaaGHVx2_?usp=share_link)**\n","\n"]},{"cell_type":"markdown","source":["#IMPORTING REQUIRED LIBRARIES"],"metadata":{"id":"TdAjBsKjdmUa"}},{"cell_type":"code","metadata":{"id":"_IehQoF0pZxl"},"source":["import librosa #sound/audio analysis\n","import soundfile #sound/audio analysis\n","import os, glob, pickle #os-directory reading, pickle-processed data is stored in pickle format\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","import shutil #file_ops\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import librosa.display\n","import IPython.display as ipd\n","from IPython.core.display import display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#DATA PREPARATION"],"metadata":{"id":"KGyV4QnDdjiI"}},{"cell_type":"markdown","source":["##RAVDESS DATASET"],"metadata":{"id":"FZdwQYlRds_z"}},{"cell_type":"code","metadata":{"id":"2b_mwxpsyugl","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1669870362450,"user_tz":-330,"elapsed":166048,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"a6e39523-9ebf-415c-f010-37e286b0b20a"},"source":["ravdess_paths = sorted(filter(os.path.isfile, glob.glob('/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/RAVDESS/*/*.wav')))\n","\n","filenames_r, duration_r, sr_r, gender_r, emotion_r = [], [], [], [], []\n","\n","for file in ravdess_paths:\n","    \n","    filenames_r.append(os.path.basename(file))\n","    \n","    duration_r.append(round(librosa.get_duration(filename=file), 3))\n","    \n","    sr_r.append(librosa.get_samplerate(file))\n","    \n","    if int(file[-6:-4])%2 == 0:\n","        gender_r.append('female')\n","    else:\n","        gender_r.append('male')\n","\n","for file in filenames_r:\n","    if file[6:8] == '01':\n","        emotion_r.append('neutral')\n","    elif file[6:8] == '02':\n","        emotion_r.append('calm')\n","    elif file[6:8] == '03':\n","        emotion_r.append('happy')\n","    elif file[6:8] == '04':\n","        emotion_r.append('sad')\n","    elif file[6:8] == '05':\n","        emotion_r.append('angry')\n","    elif file[6:8] == '06':\n","        emotion_r.append('fear')\n","    elif file[6:8] == '07':\n","        emotion_r.append('disgust')\n","    elif file[6:8] == '08':\n","        emotion_r.append('surprise')\n","    \n","df_r = pd.DataFrame({'path': ravdess_paths,\n","                   'filename': filenames_r,\n","                   'dataset' : 'RAVDESS',\n","                   'duration' : duration_r,\n","                   'sample_rate' : sr_r,\n","                   'gender': gender_r,\n","                   'age' : 26,             # from the documentation: M = 26.0 years; SD = 3.75; age range = 21â€“33, individual age data is missing\n","                   'emotion' : emotion_r})\n","\n","df_r.sample(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   path  \\\n","1062  /content/drive/MyDrive/Speech Emotion Recognit...   \n","19    /content/drive/MyDrive/Speech Emotion Recognit...   \n","1279  /content/drive/MyDrive/Speech Emotion Recognit...   \n","\n","                      filename  dataset  duration  sample_rate  gender  age  \\\n","1062  03-01-06-02-02-01-18.wav  RAVDESS     3.804        16000  female   26   \n","19    03-01-03-02-02-02-01.wav  RAVDESS     3.937        16000    male   26   \n","1279  03-01-03-02-02-02-22.wav  RAVDESS     3.937        16000  female   26   \n","\n","     emotion  \n","1062    fear  \n","19     happy  \n","1279   happy  "],"text/html":["\n","  <div id=\"df-422e47d0-57b6-4dfa-a319-567ee257f2b7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1062</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-06-02-02-01-18.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.804</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>26</td>\n","      <td>fear</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-03-02-02-02-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.937</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>1279</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-03-02-02-02-22.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.937</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>26</td>\n","      <td>happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-422e47d0-57b6-4dfa-a319-567ee257f2b7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-422e47d0-57b6-4dfa-a319-567ee257f2b7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-422e47d0-57b6-4dfa-a319-567ee257f2b7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["##TESS DATASET"],"metadata":{"id":"Fr2d1lBeQL2J"}},{"cell_type":"code","source":["tess_paths = sorted(filter(os.path.isfile, glob.glob('/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/TESS/*.wav')))\n","\n","filenames_t, duration_t, sr_t, age_t, emotion_t = [], [], [], [], []\n","\n","for file in tess_paths:\n","    \n","    filenames_t.append(os.path.basename(file))\n","    \n","    duration_t.append(round(librosa.get_duration(filename=file), 3)) \n","    \n","    sr_t.append(librosa.get_samplerate(file))\n","\n","for file in filenames_t:    \n","    if file[0:3] == 'OAF':\n","        age_t.append(64)\n","    else:\n","        age_t.append(26)\n","    splitted = file.split('_')\n","    if splitted[-1][0]=='n':\n","      emotion_t.append('calm')\n","    elif splitted[-1][0]=='h':\n","      emotion_t.append('happy')\n","    elif splitted[-1][0]=='s':\n","      emotion_t.append('sad')\n","    else:\n","      continue\n","\n","df_t = pd.DataFrame({'path': tess_paths,\n","                   'filename': filenames_t,\n","                   'dataset' : 'TESS',\n","                   'duration' : duration_t,\n","                   'sample_rate' : sr_t,\n","                   'gender': 'female',\n","                   'age' : age_t,\n","                   'emotion' : emotion_t})\n","\n","df_t.head(10)"],"metadata":{"id":"O7MbG5Q_QI-5","executionInfo":{"status":"ok","timestamp":1669870076473,"user_tz":-330,"elapsed":33609,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"aa764d28-07dc-420f-fb69-beab37560a23"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                path              filename  \\\n","0  /content/drive/MyDrive/Speech Emotion Recognit...    OAF_back_happy.wav   \n","1  /content/drive/MyDrive/Speech Emotion Recognit...  OAF_back_neutral.wav   \n","2  /content/drive/MyDrive/Speech Emotion Recognit...      OAF_back_sad.wav   \n","3  /content/drive/MyDrive/Speech Emotion Recognit...     OAF_bar_happy.wav   \n","4  /content/drive/MyDrive/Speech Emotion Recognit...   OAF_bar_neutral.wav   \n","5  /content/drive/MyDrive/Speech Emotion Recognit...       OAF_bar_sad.wav   \n","6  /content/drive/MyDrive/Speech Emotion Recognit...    OAF_base_happy.wav   \n","7  /content/drive/MyDrive/Speech Emotion Recognit...  OAF_base_neutral.wav   \n","8  /content/drive/MyDrive/Speech Emotion Recognit...      OAF_base_sad.wav   \n","9  /content/drive/MyDrive/Speech Emotion Recognit...    OAF_bath_happy.wav   \n","\n","  dataset  duration  sample_rate  gender  age emotion  \n","0    TESS     2.001        24414  female   64   happy  \n","1    TESS     2.043        24414  female   64    calm  \n","2    TESS     2.562        24414  female   64     sad  \n","3    TESS     2.000        24414  female   64   happy  \n","4    TESS     2.002        24414  female   64    calm  \n","5    TESS     2.502        24414  female   64     sad  \n","6    TESS     2.033        24414  female   64   happy  \n","7    TESS     2.150        24414  female   64    calm  \n","8    TESS     2.570        24414  female   64     sad  \n","9    TESS     2.035        24414  female   64   happy  "],"text/html":["\n","  <div id=\"df-3c195ba8-f36f-4d76-ad54-21c123b79287\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_back_happy.wav</td>\n","      <td>TESS</td>\n","      <td>2.001</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_back_neutral.wav</td>\n","      <td>TESS</td>\n","      <td>2.043</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_back_sad.wav</td>\n","      <td>TESS</td>\n","      <td>2.562</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_bar_happy.wav</td>\n","      <td>TESS</td>\n","      <td>2.000</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_bar_neutral.wav</td>\n","      <td>TESS</td>\n","      <td>2.002</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_bar_sad.wav</td>\n","      <td>TESS</td>\n","      <td>2.502</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_base_happy.wav</td>\n","      <td>TESS</td>\n","      <td>2.033</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_base_neutral.wav</td>\n","      <td>TESS</td>\n","      <td>2.150</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_base_sad.wav</td>\n","      <td>TESS</td>\n","      <td>2.570</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>OAF_bath_happy.wav</td>\n","      <td>TESS</td>\n","      <td>2.035</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>64</td>\n","      <td>happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c195ba8-f36f-4d76-ad54-21c123b79287')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3c195ba8-f36f-4d76-ad54-21c123b79287 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3c195ba8-f36f-4d76-ad54-21c123b79287');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/TESS/*.wav\"):\n","#   splitted = file.split('_')\n","#   if splitted[-1][0]=='n':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/calm\")\n","#   elif splitted[-1][0]=='h':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/happy\")\n","#   elif splitted[-1][0]=='s':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/sad\")\n","#   else:\n","#     print(1)"],"metadata":{"id":"MHbBalrdTfUQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##SAVEE DATASET"],"metadata":{"id":"ZSYdpvlzQJiP"}},{"cell_type":"code","source":["savee_paths = sorted(filter(os.path.isfile, glob.glob('/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/SAVEE/*/*.wav')))\n","\n","filenames_s, duration_s, sr_s, emotion_s = [], [], [], []\n","\n","for file in savee_paths:\n","    \n","    filenames_s.append(os.path.basename(file))\n","    \n","    duration_s.append(round(librosa.get_duration(filename=file), 3))\n","    \n","    sr_s.append(librosa.get_samplerate(file))\n","\n","for file in filenames_s:\n","    if file[0:1] == 'a':\n","        emotion_s.append('angry')\n","    elif file[0:1] == 'd':\n","        emotion_s.append('disgust')\n","    elif file[0:1] == 'f':\n","        emotion_s.append('fear')\n","    elif file[0:1] == 'h':\n","        emotion_s.append('happy')\n","    elif file[0:1] == 'n':\n","        emotion_s.append('calm')\n","    elif file[0:1] == 's':\n","        if file[0:2] == 'sa':\n","            emotion_s.append('sad')\n","        else:\n","            emotion_s.append('surprise')\n","            \n","df_s = pd.DataFrame({'path': savee_paths,\n","                   'filename': filenames_s,\n","                   'dataset' : 'SAVEE',\n","                   'duration' : duration_s,\n","                   'sample_rate' : sr_s,\n","                   'gender': 'male',\n","                   'age' : 29,            # from the documentation: age of four speaker between 27 and 31 years old, hence choosing 29\n","                   'emotion' : emotion_s})\n","\n","df_s.sample(3)"],"metadata":{"id":"Cg4r6ktgFa82","executionInfo":{"status":"ok","timestamp":1669870189522,"user_tz":-330,"elapsed":98145,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"d46217f1-cc79-4388-deb6-89034ac411cd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  path filename dataset  \\\n","254  /content/drive/MyDrive/Speech Emotion Recognit...  a15.wav   SAVEE   \n","301  /content/drive/MyDrive/Speech Emotion Recognit...  n02.wav   SAVEE   \n","273  /content/drive/MyDrive/Speech Emotion Recognit...  f04.wav   SAVEE   \n","\n","     duration  sample_rate gender  age emotion  \n","254     5.178        44100   male   29   angry  \n","301     3.962        44100   male   29    calm  \n","273     3.145        44100   male   29    fear  "],"text/html":["\n","  <div id=\"df-3480cff4-ac39-4c8c-8716-fbfbf29fe6b5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>254</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>a15.wav</td>\n","      <td>SAVEE</td>\n","      <td>5.178</td>\n","      <td>44100</td>\n","      <td>male</td>\n","      <td>29</td>\n","      <td>angry</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>n02.wav</td>\n","      <td>SAVEE</td>\n","      <td>3.962</td>\n","      <td>44100</td>\n","      <td>male</td>\n","      <td>29</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>273</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>f04.wav</td>\n","      <td>SAVEE</td>\n","      <td>3.145</td>\n","      <td>44100</td>\n","      <td>male</td>\n","      <td>29</td>\n","      <td>fear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3480cff4-ac39-4c8c-8716-fbfbf29fe6b5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3480cff4-ac39-4c8c-8716-fbfbf29fe6b5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3480cff4-ac39-4c8c-8716-fbfbf29fe6b5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["##CHREMA-D DATASET"],"metadata":{"id":"iOk9c0uSGYMu"}},{"cell_type":"code","source":["# i=0\n","# for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/CHREMA-D/*.wav\"):\n","#   splitted = os.path.basename(file).split('_')\n","#   # print(splitted[-2])\n","#   if splitted[-2]=='NEU':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/calm\")\n","#   elif splitted[-2]=='HAP':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/happy\")\n","#   elif splitted[-2]=='SAD':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/sad\")\n","#   else:\n","#     continue"],"metadata":{"id":"GigpN1mPlUMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["det = pd.read_csv('/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/VideoDemographics.csv')\n","det.head()"],"metadata":{"id":"e5hQjDAxHVS8","executionInfo":{"status":"ok","timestamp":1669870513345,"user_tz":-330,"elapsed":344,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"d79b5e73-c7e8-4a66-b58a-aed62e07049a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ActorID  Age     Sex              Race     Ethnicity\n","0     1001   51    Male         Caucasian  Not Hispanic\n","1     1002   21  Female         Caucasian  Not Hispanic\n","2     1003   21  Female         Caucasian  Not Hispanic\n","3     1004   42  Female         Caucasian  Not Hispanic\n","4     1005   29    Male  African American  Not Hispanic"],"text/html":["\n","  <div id=\"df-878a7efa-8c9c-4ade-a333-710f165d1b0c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ActorID</th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Race</th>\n","      <th>Ethnicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1001</td>\n","      <td>51</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>Not Hispanic</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1002</td>\n","      <td>21</td>\n","      <td>Female</td>\n","      <td>Caucasian</td>\n","      <td>Not Hispanic</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1003</td>\n","      <td>21</td>\n","      <td>Female</td>\n","      <td>Caucasian</td>\n","      <td>Not Hispanic</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1004</td>\n","      <td>42</td>\n","      <td>Female</td>\n","      <td>Caucasian</td>\n","      <td>Not Hispanic</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1005</td>\n","      <td>29</td>\n","      <td>Male</td>\n","      <td>African American</td>\n","      <td>Not Hispanic</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-878a7efa-8c9c-4ade-a333-710f165d1b0c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-878a7efa-8c9c-4ade-a333-710f165d1b0c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-878a7efa-8c9c-4ade-a333-710f165d1b0c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["crema_d_paths = sorted(filter(os.path.isfile, glob.glob('/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/CHREMA-D/*.wav')))\n","filenames_c, duration_c, sr_c, gender_c, age_c, emotion_c = [], [], [], [], [], []\n","\n","# iterate through each file path and extract metadata\n","for file in crema_d_paths:\n","    # get filenames using slicing string\n","    fname=os.path.basename(file)\n","    filenames_c.append(fname)\n","    # get audio lenght, in seconds\n","    duration_c.append(round(librosa.get_duration(filename=file), 3))\n","    # get sample rate\n","    sr_c.append(librosa.get_samplerate(file))\n","    # get id of the speaker\n","    actor = int(os.path.basename(file)[0:4])\n","    # get gender\n","    gender_c.append(det[det['ActorID'] == actor]['Sex'].item().lower())\n","    # get age\n","    age_c.append(det[det['ActorID'] == actor]['Age'].item())\n","    # get emotion using slicing strings\n","    if fname[9:12] == 'SAD':\n","        emotion_c.append('sad')\n","    elif fname[9:12] == 'ANG':\n","        emotion_c.append('angry')\n","    elif fname[9:12] == 'DIS':\n","        emotion_c.append('disgust')\n","    elif fname[9:12] == 'FEA':\n","        emotion_c.append('fear')\n","    elif fname[9:12] == 'HAP':\n","        emotion_c.append('happy')\n","    elif fname[9:12] == 'NEU':\n","        emotion_c.append('neutral')\n","\n","# create dataframe\n","df_c = pd.DataFrame({'path': crema_d_paths,\n","                   'filename': filenames_c,\n","                   'dataset' : 'CREMA-D',\n","                   'duration' : duration_c,\n","                   'sample_rate' : sr_c,\n","                   'gender': gender_c,\n","                   'age' : age_c,\n","                   'emotion' : emotion_c})\n","df_c.sample(10)"],"metadata":{"id":"gsT_1oPp9_H6","executionInfo":{"status":"ok","timestamp":1669870655726,"user_tz":-330,"elapsed":141051,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"824db5fe-5c77-481b-ff56-2c8dd5a132c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   path             filename  \\\n","790   /content/drive/MyDrive/Speech Emotion Recognit...  1010_TIE_DIS_XX.wav   \n","215   /content/drive/MyDrive/Speech Emotion Recognit...  1003_MTI_ANG_XX.wav   \n","1467  /content/drive/MyDrive/Speech Emotion Recognit...  1019_DFA_NEU_XX.wav   \n","4655  /content/drive/MyDrive/Speech Emotion Recognit...  1058_DFA_ANG_XX.wav   \n","2457  /content/drive/MyDrive/Speech Emotion Recognit...  1031_IEO_HAP_LO.wav   \n","1216  /content/drive/MyDrive/Speech Emotion Recognit...  1015_WSI_SAD_XX.wav   \n","965   /content/drive/MyDrive/Speech Emotion Recognit...  1012_WSI_ANG_XX.wav   \n","4888  /content/drive/MyDrive/Speech Emotion Recognit...  1060_TIE_SAD_XX.wav   \n","1407  /content/drive/MyDrive/Speech Emotion Recognit...  1018_IOM_NEU_XX.wav   \n","416   /content/drive/MyDrive/Speech Emotion Recognit...  1006_IEO_ANG_LO.wav   \n","\n","      dataset  duration  sample_rate  gender  age  emotion  \n","790   CREMA-D     3.937        16000  female   27  disgust  \n","215   CREMA-D     2.236        16000  female   21    angry  \n","1467  CREMA-D     1.568        16000    male   29  neutral  \n","4655  CREMA-D     2.469        16000  female   36    angry  \n","2457  CREMA-D     1.768        16000    male   31    happy  \n","1216  CREMA-D     2.436        16000    male   32      sad  \n","965   CREMA-D     2.469        16000  female   23    angry  \n","4888  CREMA-D     3.437        16000  female   28      sad  \n","1407  CREMA-D     1.969        16000  female   25  neutral  \n","416   CREMA-D     2.502        16000  female   58    angry  "],"text/html":["\n","  <div id=\"df-b82bb7c1-8834-41cd-bdd8-7cec0f0a0b09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>790</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1010_TIE_DIS_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>3.937</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>27</td>\n","      <td>disgust</td>\n","    </tr>\n","    <tr>\n","      <th>215</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1003_MTI_ANG_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.236</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>21</td>\n","      <td>angry</td>\n","    </tr>\n","    <tr>\n","      <th>1467</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1019_DFA_NEU_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>1.568</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>29</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4655</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1058_DFA_ANG_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.469</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>36</td>\n","      <td>angry</td>\n","    </tr>\n","    <tr>\n","      <th>2457</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1031_IEO_HAP_LO.wav</td>\n","      <td>CREMA-D</td>\n","      <td>1.768</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>31</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>1216</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1015_WSI_SAD_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.436</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>32</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>965</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1012_WSI_ANG_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.469</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>23</td>\n","      <td>angry</td>\n","    </tr>\n","    <tr>\n","      <th>4888</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1060_TIE_SAD_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>3.437</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>28</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>1407</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1018_IOM_NEU_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>1.969</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>25</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>416</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1006_IEO_ANG_LO.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.502</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>58</td>\n","      <td>angry</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b82bb7c1-8834-41cd-bdd8-7cec0f0a0b09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b82bb7c1-8834-41cd-bdd8-7cec0f0a0b09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b82bb7c1-8834-41cd-bdd8-7cec0f0a0b09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["##Cummilative Data"],"metadata":{"id":"8lLaUpJyd22o"}},{"cell_type":"code","source":["# Merge all the datasets together\n","df = pd.concat([df_r, df_s, df_t, df_c])\n","\n","# Get few random entries\n","df.sample(10)"],"metadata":{"id":"rVu-oPUaWX3J","executionInfo":{"status":"ok","timestamp":1669870655727,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"bf05326a-f67b-4f71-f655-fe32606fce99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   path  \\\n","127   /content/drive/MyDrive/Speech Emotion Recognit...   \n","2363  /content/drive/MyDrive/Speech Emotion Recognit...   \n","10    /content/drive/MyDrive/Speech Emotion Recognit...   \n","1283  /content/drive/MyDrive/Speech Emotion Recognit...   \n","4369  /content/drive/MyDrive/Speech Emotion Recognit...   \n","3011  /content/drive/MyDrive/Speech Emotion Recognit...   \n","313   /content/drive/MyDrive/Speech Emotion Recognit...   \n","3702  /content/drive/MyDrive/Speech Emotion Recognit...   \n","841   /content/drive/MyDrive/Speech Emotion Recognit...   \n","4923  /content/drive/MyDrive/Speech Emotion Recognit...   \n","\n","                      filename  dataset  duration  sample_rate  gender  age  \\\n","127        1002_IWL_SAD_XX.wav  CREMA-D     2.703        16000  female   21   \n","2363       1030_DFA_NEU_XX.wav  CREMA-D     2.402        16000  female   42   \n","10         1001_IEO_DIS_LO.wav  CREMA-D     2.002        16000    male   51   \n","1283  03-01-04-01-02-02-22.wav  RAVDESS     3.704        16000  female   26   \n","4369       1054_IWL_FEA_XX.wav  CREMA-D     2.536        16000  female   36   \n","3011       1037_WSI_FEA_XX.wav  CREMA-D     2.669        16000  female   45   \n","313   03-01-03-01-01-02-06.wav  RAVDESS     3.604        16000  female   26   \n","3702       1046_ITH_HAP_XX.wav  CREMA-D     2.102        16000  female   22   \n","841        YAF_learn_happy.wav     TESS     2.081        24414  female   26   \n","4923       1061_IOM_ANG_XX.wav  CREMA-D     2.569        16000  female   51   \n","\n","      emotion  \n","127       sad  \n","2363  neutral  \n","10    disgust  \n","1283      sad  \n","4369     fear  \n","3011     fear  \n","313     happy  \n","3702    happy  \n","841     happy  \n","4923    angry  "],"text/html":["\n","  <div id=\"df-b63b0482-2715-4d55-8d06-ca474efc07fc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>127</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1002_IWL_SAD_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.703</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>21</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>2363</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1030_DFA_NEU_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.402</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>42</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1001_IEO_DIS_LO.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.002</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>51</td>\n","      <td>disgust</td>\n","    </tr>\n","    <tr>\n","      <th>1283</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-04-01-02-02-22.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.704</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>26</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>4369</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1054_IWL_FEA_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.536</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>36</td>\n","      <td>fear</td>\n","    </tr>\n","    <tr>\n","      <th>3011</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1037_WSI_FEA_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.669</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>45</td>\n","      <td>fear</td>\n","    </tr>\n","    <tr>\n","      <th>313</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-03-01-01-02-06.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.604</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>26</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>3702</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1046_ITH_HAP_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.102</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>22</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>841</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>YAF_learn_happy.wav</td>\n","      <td>TESS</td>\n","      <td>2.081</td>\n","      <td>24414</td>\n","      <td>female</td>\n","      <td>26</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>4923</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1061_IOM_ANG_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.569</td>\n","      <td>16000</td>\n","      <td>female</td>\n","      <td>51</td>\n","      <td>angry</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b63b0482-2715-4d55-8d06-ca474efc07fc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b63b0482-2715-4d55-8d06-ca474efc07fc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b63b0482-2715-4d55-8d06-ca474efc07fc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["df['emotion'].value_counts()"],"metadata":{"id":"EAe8gJIKaPMH","executionInfo":{"status":"ok","timestamp":1669870655727,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a26f67f4-f563-4f3c-8ded-4bd892300f45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["happy       1515\n","sad         1507\n","angry       1116\n","fear        1116\n","disgust     1116\n","neutral      834\n","calm         709\n","surprise     252\n","Name: emotion, dtype: int64"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["df = df[df['emotion'].str.contains('angry|fear|disgust|surprise') == False].reset_index(drop=True)"],"metadata":{"id":"D_O1QEXbaKbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.replace({'emotion':{'neutral':'calm'}},inplace=True)"],"metadata":{"id":"akV-3FcVbGPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['emotion'].value_counts()"],"metadata":{"id":"Hhc_J5m3awtz","executionInfo":{"status":"ok","timestamp":1669870655728,"user_tz":-330,"elapsed":10,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"13dd821a-7701-4e7a-b254-2f8836c7ced8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["calm     1543\n","happy    1515\n","sad      1507\n","Name: emotion, dtype: int64"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"ChY98p4Ibl54","executionInfo":{"status":"ok","timestamp":1669870655728,"user_tz":-330,"elapsed":8,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5667cfb1-5ea7-4ef7-c0c2-268f8cc68ccc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4565 entries, 0 to 4564\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   path         4565 non-null   object \n"," 1   filename     4565 non-null   object \n"," 2   dataset      4565 non-null   object \n"," 3   duration     4565 non-null   float64\n"," 4   sample_rate  4565 non-null   int64  \n"," 5   gender       4565 non-null   object \n"," 6   age          4565 non-null   int64  \n"," 7   emotion      4565 non-null   object \n","dtypes: float64(1), int64(2), object(5)\n","memory usage: 285.4+ KB\n"]}]},{"cell_type":"markdown","source":["#Modelling"],"metadata":{"id":"OM6zg-A2eHRw"}},{"cell_type":"markdown","source":["##Classical ML"],"metadata":{"id":"K8LIMTcX2jqg"}},{"cell_type":"code","metadata":{"id":"iqRKYAdhdU18"},"source":["#Extract features (mfcc, chroma, mel) from a sound file\n","def extract_feature(file_name): # different frequency formats \n","    with soundfile.SoundFile(file_name) as sound_file:\n","        X = sound_file.read(dtype=\"float32\")\n","        sample_rate=sound_file.samplerate #The audio sample rate determines the range of frequencies captured in digital audio.\n","        if True:\n","            stft=np.abs(librosa.stft(X)) #Short-time Fourier transform - The STFT represents a signal in the time-frequency domain by computing discrete Fourier transforms (DFT) over short overlapping windows.\n","        result=np.array([])\n","        if True:\n","            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0) #mfcc-Mel-frequency cepstral coefficients (MFCCs)\n","            result=np.hstack((result, mfccs))\n","        if True:\n","            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0) #Compute a chromagram from a waveform or power spectrogram.\n","            result=np.hstack((result, chroma))\n","        if True:\n","            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0) #Compute a mel-scaled spectrogram.\n","            result=np.hstack((result, mel))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqDcKpB_dU18"},"source":["#Emotions to observe\n","observed_emotions=['calm', 'happy', 'sad']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCsFr-scdU18"},"source":["#Load the data and extract features for each sound file\n","def load_data(test_size=0.2):\n","    x,y=[],[]\n","    for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/calm/*.wav\"):\n","        file_name=os.path.basename(file)\n","        emotion=df.loc[df.filename == file_name,'emotion'].iloc[0]\n","        if emotion not in observed_emotions:\n","            continue\n","        feature=extract_feature(file)\n","        x.append(feature)\n","        y.append(emotion)\n","    for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/happy/*.wav\"):\n","        file_name=os.path.basename(file)\n","        emotion=df.loc[df.filename == file_name,'emotion'].iloc[0]\n","        if emotion not in observed_emotions:\n","            continue\n","        feature=extract_feature(file)\n","        x.append(feature)\n","        y.append(emotion)\n","    for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/sad/*.wav\"):\n","        file_name=os.path.basename(file)\n","        emotion=df.loc[df.filename == file_name,'emotion'].iloc[0]\n","        if emotion not in observed_emotions:\n","            continue\n","        feature=extract_feature(file)\n","        x.append(feature)\n","        y.append(emotion)\n","    return train_test_split(np.array(x), y, test_size=test_size, random_state=2401)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# os.mkdir(os.path.join(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data\",\"happy\"))\n","# os.mkdir(os.path.join(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data\",\"sad\"))\n","# os.mkdir(os.path.join(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data\",\"calm\"))"],"metadata":{"id":"EDzUbzy1dU18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for file in glob.glob(\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/RAVDESS/Actor_*/*.wav\"):\n","#   file_name=os.path.basename(file)\n","#   emotion=emotions[file_name.split(\"-\")[2]]\n","#   if emotion=='happy':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/happy\")\n","#   elif emotion=='sad':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/sad\")\n","#   elif emotion=='calm':\n","#     shutil.copy(os.path.abspath(file),\"/content/drive/MyDrive/Speech Emotion Recognition/speech-emotion-recognition-data/calm\")\n","#   else:\n","#     continue"],"metadata":{"id":"0J7whauOdU19"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Is1fjavSdU1-"},"source":["#Split the dataset\n","x_train,x_test,y_train,y_test=load_data(test_size=0.20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1669871457823,"user_tz":-330,"elapsed":490,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"id":"bKHxlZTcdU1-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e4838d3-0230-47c6-e387-4611dfb1866d"},"source":["x_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-2.66404907e+02,  1.03220589e+02,  7.38228798e+00, ...,\n","         1.08810503e-03,  4.48347884e-04,  4.79550858e-04],\n","       [-2.95373718e+02,  1.12220245e+02,  2.83583126e+01, ...,\n","         6.23477972e-04,  8.17711581e-04,  3.77891702e-04],\n","       [-3.05417053e+02,  8.28992538e+01,  3.78992157e+01, ...,\n","         1.73094999e-02,  8.81935284e-03,  8.56737420e-03],\n","       ...,\n","       [-3.55005951e+02,  1.00322708e+02,  4.08377304e+01, ...,\n","         1.34860061e-03,  6.64582243e-04,  6.06165209e-04],\n","       [-3.65334320e+02,  1.07500015e+02,  3.32819290e+01, ...,\n","         1.38765274e-04,  9.61900441e-05,  1.08311884e-04],\n","       [-3.98337952e+02,  1.04025330e+02,  4.10417480e+01, ...,\n","         2.71429250e-04,  1.94357490e-04,  2.28176505e-04]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1669871458381,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"id":"bR3gIseJdU1-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd331eb7-4b63-4fd7-91f8-dbb44a708d35"},"source":["#Get the shape of the training and testing datasets\n","print((x_train.shape, x_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["((3431, 180), (858, 180))\n"]}]},{"cell_type":"code","metadata":{"id":"_jjw2xRUdU1-"},"source":["# Initialize the Multi Layer Perceptron Classifier\n","model=MLPClassifier(activation='tanh', alpha=0.01, batch_size=256,\n","              early_stopping=True, hidden_layer_sizes=(300,),\n","              learning_rate='adaptive', max_iter=2000, shuffle=True) #78"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.ensemble import RandomForestClassifier\n","# model=RandomForestClassifier(n_estimators=2500, criterion='gini') 77"],"metadata":{"id":"rHx1y9P9RqQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.svm import SVC\n","# model=SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n","#           probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n","#           max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None) 71"],"metadata":{"id":"-c1ayYH9zzDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1669871485312,"user_tz":-330,"elapsed":6684,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"id":"eubtfcwcdU1_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2fc353b-156e-49da-e671-875b4057bf16"},"source":["#Train the model\n","model.fit(x_train,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='tanh', alpha=0.01, batch_size=256,\n","              early_stopping=True, hidden_layer_sizes=(300,),\n","              learning_rate='adaptive', max_iter=2000)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"h3JgxXpvdU1_"},"source":["#Predict for the test set\n","y_pred=model.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zlk73segdU1_"},"source":["# y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1F0fvxWxdU1_"},"source":["from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1669871491210,"user_tz":-330,"elapsed":341,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"id":"M8ZiCsXtdU1_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"314c1fd4-7f75-4344-9087-29b860256a7f"},"source":["#Calculate the accuracy of our model\n","accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n","\n","#Print the accuracy\n","print(\"Accuracy: {:.2f} %\".format(accuracy*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 78.44 %\n"]}]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1669871497000,"user_tz":-330,"elapsed":347,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"id":"IK8Y3GBYdU1_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a76d755-66e6-44a0-e6df-a1f47908e763"},"source":["f1_score(y_test, y_pred,average=None)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.74616695, 0.83531409, 0.77037037])"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["confusion_matrix(y_test,y_pred)"],"metadata":{"id":"FUhmguz9ncT1","executionInfo":{"status":"ok","timestamp":1669871498586,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f8eab4f-d0c9-4539-d80a-8b8b0571e757"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[219,  29,  48],\n","       [ 32, 246,  18],\n","       [ 40,  18, 208]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n","fig, ax = plt.subplots(figsize=(7.5, 7.5))\n","ax.matshow(conf_matrix, cmap='RdYlBu', alpha=0.7)\n","for i in range(conf_matrix.shape[0]):\n","    for j in range(conf_matrix.shape[1]):\n","        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n"," \n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"eyL-MuW-nrrd","executionInfo":{"status":"ok","timestamp":1669871502104,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/","height":501},"outputId":"7fc13272-f04a-4a69-96ea-525276766963"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 540x540 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"metadata":{"id":"ZyYJMhMXooWq","executionInfo":{"status":"ok","timestamp":1669871507422,"user_tz":-330,"elapsed":10,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b463f42-dcef-43d6-a858-62da01f2ccc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        calm       0.75      0.74      0.75       296\n","       happy       0.84      0.83      0.84       296\n","         sad       0.76      0.78      0.77       266\n","\n","    accuracy                           0.78       858\n","   macro avg       0.78      0.78      0.78       858\n","weighted avg       0.78      0.78      0.78       858\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGw1yL7ESS_v"},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# max_features=np.arange(1,100,1)\n","# n_estimators=np.arange(10,1000,10)\n","# param_grid=dict(max_features=max_features,n_estimators=n_estimators)\n","\n","# grid=GridSearchCV(estimator=model,param_grid=param_grid,cv=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUQYiZXTZJ_k"},"outputs":[],"source":["# grid.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATMJzyCdZX0k"},"outputs":[],"source":["# print(\"The best parameters are %s with a score %.2f\"%(grid.best_params_,grid.best_score_))"]},{"cell_type":"code","source":[],"metadata":{"id":"EvWAfwEfQ0Cx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##DL"],"metadata":{"id":"tkVkHGxG2mMn"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"38Pl_YSm3ItG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_dl=df\n","df_dl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"pYUOOGI_27w2","executionInfo":{"status":"ok","timestamp":1669871818872,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"771138de-b65a-4071-ed62-b8dc156e512b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   path  \\\n","0     /content/drive/MyDrive/Speech Emotion Recognit...   \n","1     /content/drive/MyDrive/Speech Emotion Recognit...   \n","2     /content/drive/MyDrive/Speech Emotion Recognit...   \n","3     /content/drive/MyDrive/Speech Emotion Recognit...   \n","4     /content/drive/MyDrive/Speech Emotion Recognit...   \n","...                                                 ...   \n","4560  /content/drive/MyDrive/Speech Emotion Recognit...   \n","4561  /content/drive/MyDrive/Speech Emotion Recognit...   \n","4562  /content/drive/MyDrive/Speech Emotion Recognit...   \n","4563  /content/drive/MyDrive/Speech Emotion Recognit...   \n","4564  /content/drive/MyDrive/Speech Emotion Recognit...   \n","\n","                      filename  dataset  duration  sample_rate gender  age  \\\n","0     03-01-01-01-01-01-01.wav  RAVDESS     3.303        16000   male   26   \n","1     03-01-01-01-01-02-01.wav  RAVDESS     3.337        16000   male   26   \n","2     03-01-01-01-02-01-01.wav  RAVDESS     3.270        16000   male   26   \n","3     03-01-01-01-02-02-01.wav  RAVDESS     3.170        16000   male   26   \n","4     03-01-02-01-01-01-01.wav  RAVDESS     3.537        16000   male   26   \n","...                        ...      ...       ...          ...    ...  ...   \n","4560       1062_TAI_NEU_XX.wav  CREMA-D     1.902        16000   male   56   \n","4561       1062_TAI_SAD_XX.wav  CREMA-D     2.302        16000   male   56   \n","4562       1062_TIE_HAP_XX.wav  CREMA-D     2.569        16000   male   56   \n","4563       1062_TIE_NEU_XX.wav  CREMA-D     2.302        16000   male   56   \n","4564       1062_TIE_SAD_XX.wav  CREMA-D     2.803        16000   male   56   \n","\n","     emotion  \n","0       calm  \n","1       calm  \n","2       calm  \n","3       calm  \n","4       calm  \n","...      ...  \n","4560    calm  \n","4561     sad  \n","4562   happy  \n","4563    calm  \n","4564     sad  \n","\n","[4565 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-af822241-ee62-4fbd-8ee6-a705d7462ee2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>filename</th>\n","      <th>dataset</th>\n","      <th>duration</th>\n","      <th>sample_rate</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-01-01-01-01-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.303</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-01-01-01-02-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.337</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-01-01-02-01-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.270</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-01-01-02-02-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.170</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>03-01-02-01-01-01-01.wav</td>\n","      <td>RAVDESS</td>\n","      <td>3.537</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>26</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4560</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1062_TAI_NEU_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>1.902</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>56</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>4561</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1062_TAI_SAD_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.302</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>56</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>4562</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1062_TIE_HAP_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.569</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>56</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>4563</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1062_TIE_NEU_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.302</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>56</td>\n","      <td>calm</td>\n","    </tr>\n","    <tr>\n","      <th>4564</th>\n","      <td>/content/drive/MyDrive/Speech Emotion Recognit...</td>\n","      <td>1062_TIE_SAD_XX.wav</td>\n","      <td>CREMA-D</td>\n","      <td>2.803</td>\n","      <td>16000</td>\n","      <td>male</td>\n","      <td>56</td>\n","      <td>sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4565 rows Ã— 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af822241-ee62-4fbd-8ee6-a705d7462ee2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-af822241-ee62-4fbd-8ee6-a705d7462ee2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-af822241-ee62-4fbd-8ee6-a705d7462ee2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["df_dl.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZporWY62ts5","executionInfo":{"status":"ok","timestamp":1669871821830,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"8bdb4498-7d21-4950-9ce1-4b4076fd1794"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4565 entries, 0 to 4564\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   path         4565 non-null   object \n"," 1   filename     4565 non-null   object \n"," 2   dataset      4565 non-null   object \n"," 3   duration     4565 non-null   float64\n"," 4   sample_rate  4565 non-null   int64  \n"," 5   gender       4565 non-null   object \n"," 6   age          4565 non-null   int64  \n"," 7   emotion      4565 non-null   object \n","dtypes: float64(1), int64(2), object(5)\n","memory usage: 285.4+ KB\n"]}]},{"cell_type":"code","source":["df_dl.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HICZqxGo32rv","executionInfo":{"status":"ok","timestamp":1669871824092,"user_tz":-330,"elapsed":602,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"d54ee512-1058-49a1-c9f4-3d12b973b70c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['path', 'filename', 'dataset', 'duration', 'sample_rate', 'gender',\n","       'age', 'emotion'],\n","      dtype='object')"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["df_dl['emotion'].replace({'happy':1,'calm':0,'sad':2}, inplace=True)\n","df_dl['gender'].replace({'female':0,'male':1}, inplace=True)"],"metadata":{"id":"29Eco5943T1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df_dl[['sample_rate', 'gender']]\n","y = df_dl.emotion"],"metadata":{"id":"jLUb8j8h3jg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.sample_rate.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wa772JbHkSg","executionInfo":{"status":"ok","timestamp":1669871877859,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"db9610b1-888a-489d-a6c7-06d79f069adf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16000    3136\n","24414    1189\n","44100     240\n","Name: sample_rate, dtype: int64"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, stratify=y, random_state=2401)"],"metadata":{"id":"i2rlOYZJ3-Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape, y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyYGEOE54dQW","executionInfo":{"status":"ok","timestamp":1669873875284,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"117db77a-8c66-4644-dee3-42b63be72916"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4108, 2), (4108,))"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["# Scale the data\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create Standard scaler\n","scaler = StandardScaler()\n","\n","# Rescale data\n","X_train_s = scaler.fit_transform(X_train)\n","X_test_s = scaler.transform(X_test)"],"metadata":{"id":"aXPIxVxm5L9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XMkR5M47XhU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669873912952,"user_tz":-330,"elapsed":12141,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"5ec85ef8-04d9-40af-c2a0-18ff71cce903"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/101\n","219/219 [==============================] - 1s 3ms/step - loss: 1.7860 - acc: 0.2781 - val_loss: 1.5423 - val_acc: 0.3420\n","Epoch 2/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.4078 - acc: 0.3432 - val_loss: 1.3111 - val_acc: 0.3485\n","Epoch 3/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.2487 - acc: 0.3386 - val_loss: 1.2067 - val_acc: 0.3420\n","Epoch 4/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1792 - acc: 0.3435 - val_loss: 1.1593 - val_acc: 0.3549\n","Epoch 5/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1474 - acc: 0.3463 - val_loss: 1.1363 - val_acc: 0.3549\n","Epoch 6/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1303 - acc: 0.3406 - val_loss: 1.1240 - val_acc: 0.3420\n","Epoch 7/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1205 - acc: 0.3331 - val_loss: 1.1163 - val_acc: 0.3420\n","Epoch 8/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1148 - acc: 0.3317 - val_loss: 1.1113 - val_acc: 0.3614\n","Epoch 9/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1113 - acc: 0.3432 - val_loss: 1.1104 - val_acc: 0.3193\n","Epoch 10/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1083 - acc: 0.3500 - val_loss: 1.1066 - val_acc: 0.3517\n","Epoch 11/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1067 - acc: 0.3443 - val_loss: 1.1056 - val_acc: 0.3420\n","Epoch 12/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1060 - acc: 0.3334 - val_loss: 1.1058 - val_acc: 0.3387\n","Epoch 13/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1045 - acc: 0.3349 - val_loss: 1.1018 - val_acc: 0.3549\n","Epoch 14/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1041 - acc: 0.3357 - val_loss: 1.1009 - val_acc: 0.3549\n","Epoch 15/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1030 - acc: 0.3460 - val_loss: 1.1016 - val_acc: 0.3420\n","Epoch 16/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1025 - acc: 0.3412 - val_loss: 1.1026 - val_acc: 0.3339\n","Epoch 17/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1026 - acc: 0.3374 - val_loss: 1.1034 - val_acc: 0.3339\n","Epoch 18/101\n","219/219 [==============================] - 0s 2ms/step - loss: 1.1018 - acc: 0.3406 - val_loss: 1.1002 - val_acc: 0.3485\n"]}],"source":["model=tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Dense(8,\"tanh\",input_dim=X_train_s.shape[1]))\n","model.add(tf.keras.layers.Dense(16,\"relu\"))\n","model.add(tf.keras.layers.Dense(7,\"softmax\"))\n","model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n","\n","epochs=101\n","history=model.fit(X_train_s, y_train, epochs=epochs, batch_size = 16,validation_split=0.15, shuffle=True, callbacks = [tf.keras.callbacks.EarlyStopping(\n","    monitor='val_acc',\n","    patience=10,\n","    restore_best_weights=True\n",")])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0-OL0VqBESe"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRudBFDeZJgk"},"outputs":[],"source":["model.evaluate(X_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyDqBMKnZMhX"},"outputs":[],"source":["plt.figure(figsize=(12,6))\n","plt.plot(range(epochs),history.history['accuracy'],label='Training Accuracy',color='r')\n","plt.plot(range(epochs),history.history['val_accuracy'],label='Validation Accuracy',color='black')\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","\n","plt.legend()\n","plt.title('Accuracy v/s Time')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UD4vlQpM7Ybb"},"source":["#CNN"]},{"cell_type":"code","source":["model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Embedding(90000, 32, input_length=X_train_s.shape[1]))\n","model.add(tf.keras.layers.SpatialDropout1D(0.2))\n","model.add(tf.keras.layers.Conv1D(256,5,activation='tanh',padding='same'))\n","model.add(tf.keras.layers.SpatialDropout1D(0.2))\n","model.add(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n","model.add(tf.keras.layers.Conv1D(256,3,activation='relu',padding='same'))\n","model.add(tf.keras.layers.SpatialDropout1D(0.2))\n","model.add(tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(7,activation='softmax'))\n","model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer=keras.optimizers.Adam(learning_rate=0.002529), metrics=[\"acc\"])\n","model.summary()"],"metadata":{"id":"n1rJh5KDpbuV","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1669874978315,"user_tz":-330,"elapsed":348,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"09bfe405-4e0a-467e-a38a-b900ac8d69cf"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-245998c20006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    229\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d_10\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)"]}]},{"cell_type":"code","source":["epochs=1000\n","history=model.fit(X_train_s, y_train, epochs=epochs, batch_size = 16,validation_split=0.15,callbacks = [tf.keras.callbacks.EarlyStopping(\n","    monitor='val_acc',\n","    min_delta=0,\n","    patience=10,\n","    verbose=0,\n","    mode='auto',\n","    baseline=None,\n","    restore_best_weights=False\n",")])"],"metadata":{"id":"VO-EMg-2pcMs","colab":{"base_uri":"https://localhost:8080/","height":484},"executionInfo":{"status":"error","timestamp":1669874453117,"user_tz":-330,"elapsed":36493,"user":{"displayName":"Shreejit Cheela","userId":"07517081544599041860"}},"outputId":"fa05a82e-e28f-46d2-dde3-a4e6e596f23c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","219/219 [==============================] - 10s 46ms/step - loss: 1.1018 - acc: 0.3457 - val_loss: 1.1091 - val_acc: 0.3387\n","Epoch 2/1000\n","219/219 [==============================] - 12s 53ms/step - loss: 1.0999 - acc: 0.3500 - val_loss: 1.0972 - val_acc: 0.3339\n","Epoch 3/1000\n","210/219 [===========================>..] - ETA: 0s - loss: 1.1005 - acc: 0.3494"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-2b363b452dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history=model.fit(X_train_s, y_train, epochs=epochs, batch_size = 16,validation_split=0.15,callbacks = [tf.keras.callbacks.EarlyStopping(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model.evaluate(X_test_s,y_test)"],"metadata":{"id":"V1gCp_BApcAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","epochs = len(history.history['binary_accuracy'])\n","plt.plot(range(epochs),history.history['binary_accuracy'],label='Training Accuracy',color='r')\n","plt.plot(range(epochs),history.history['val_binary_accuracy'],label='Validation Accuracy',color='black')\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","\n","plt.legend()\n","plt.title('Accuracy v/s Time')\n","\n","plt.show()"],"metadata":{"id":"t3gxYI_Bpb9k"},"execution_count":null,"outputs":[]}]}